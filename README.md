# Proyecto 1 - Modelos de Sistemas  
### Universidad de Antioquia - Ingenier√≠a de Sistemas (Modalidad Virtual)

---

## Descripci√≥n  
Este repositorio corresponde al **Proyecto 1** de la asignatura *Modelos de Sistemas*.  

El proyecto consiste en desarrollar un modelo de Machine Learning para predecir el rendimiento acad√©mico en las **Pruebas Saber Pro de Colombia**, utilizando variables socioecon√≥micas y acad√©micas de estudiantes universitarios.

**Competencia Kaggle:** [UDEA AI 4 - Pruebas Saber Pro Colombia](https://www.kaggle.com/competitions/udea-ai-4-eng-20252-pruebas-saber-pro-colombia)

---

## Integrantes del equipo  

| Nombre | C√©dula | Programa | Usuario Kaggle |
|--------|--------|----------|----------------|
| **Jos√© David Henao Gallego** | 1002205747 | Ingenier√≠a de Sistemas | [josdhenaogallego](https://www.kaggle.com/josdhenaogallego) |
| **Juan Andr√©s Lema Tamayo** | 1001233264 | Ingenier√≠a de Sistemas | [juanandreslematamayo](https://www.kaggle.com/juanandreslematamayo) |

---

## Contenido del repositorio  
```
Proyecto-1-Modelos-de-Sistemas/
‚îÇ
‚îú‚îÄ‚îÄ 01 - exploraci√≥n.ipynb          # An√°lisis exploratorio de datos
‚îú‚îÄ‚îÄ 02 - preprocesado.ipynb          # Preprocesamiento y entrenamiento del modelo
‚îú‚îÄ‚îÄ 03 - modelo con preprocesado LogisticRegression OneHot SimpleImputer.ipynb
‚îú‚îÄ‚îÄ 04 - modelo con preprocesado Ensemble GradientBoosting XGBoost LightGBM OneHot.ipynb
‚îú‚îÄ‚îÄ 05 - modelo con preprocesado Stacking XGB LGBM GB meta LogReg OrdinalEncoder.ipynb
‚îú‚îÄ‚îÄ 06 - modelo con preprocesado LGBM optimizado OOF FeatEng Ordinal.ipynb
‚îú‚îÄ‚îÄ 07 - modelo soluci√≥n.ipynb  # MODELO FINAL
‚îú‚îÄ‚îÄ README.md                   # Este archivo                 
```

---

## Entregables - Entrega 2

### 1. Video Explicativo (3-4 minutos)

**[Ver video en YouTube](https://youtu.be/jmIibmWRDDE)**

### Opci√≥n 2 - Drive:
** [Ver en Drive](https://drive.google.com/file/d/1pVYYp-RLQFH_aEdL70CAZgdKqutwZ4ZT/view?usp=sharing)**

**Contenido del video:**
- Explicaci√≥n del preprocesamiento de datos
- Transformaciones aplicadas (one-hot encoding, normalizaci√≥n, etc.)
- Demostraci√≥n del notebook `02 - preprocesado.ipynb`

---

### 2. Notebook de Preprocesamiento

**Archivo:** `02 - preprocesado.ipynb`

**Operaciones realizadas:**

#### **Carga de datos**
- Dataset de entrenamiento: 692,500 registros √ó 21 columnas
- Dataset de prueba: 296,786 registros √ó 20 columnas

#### **Limpieza de datos**
- Manejo de valores faltantes (3-4% del dataset)
- Reemplazo de nulos por categor√≠a `'no info'`
- Limpieza de categor√≠as ambiguas (`'No sabe'`, `'No Aplica'`)

#### **Transformaciones aplicadas**

1. **Conversi√≥n de rangos num√©ricos** (`E_VALORMATRICULAUNIVERSIDAD`)
```python
   'Menos de 500 mil' ‚Üí 0.25
   'Entre 1-2.5 millones' ‚Üí 1.75
   'M√°s de 7 millones' ‚Üí 8.0
```

2. **Codificaci√≥n One-Hot** (Variables categ√≥ricas)
   - `F_EDUCACIONMADRE` ‚Üí 11 columnas binarias
   - `F_EDUCACIONPADRE` ‚Üí 11 columnas binarias
   - `E_PAGOMATRICULAPROPIO` ‚Üí Variable binaria

3. **Codificaci√≥n ordinal del target** (`RENDIMIENTO_GLOBAL`)
```python
   'bajo' ‚Üí 0
   'medio-bajo' ‚Üí 1
   'medio-alto' ‚Üí 2
   'alto' ‚Üí 3
```

---

## Entregables - Entrega 3

### 1. Video Explicativo Final (3-4 minutos)

**[Ver video en YouTube](ENLACE_PENDIENTE)**

**Opci√≥n 2 - Drive:**
**[Ver en Drive](ENLACE_PENDIENTE)**

**Contenido del video:**
- Evoluci√≥n de los modelos (del baseline al ensemble final)
- Explicaci√≥n del ensemble ponderado con LightGBM, XGBoost y CatBoost
- Resultados en Kaggle (Score: 0.44355)
- Demostraci√≥n del notebook `99_modelo_solucion_ensemble_LGBM_XGB_CatBoost.ipynb`

---

### 2. Evoluci√≥n de los modelos

#### üìä **Modelo 3: Baseline - Logistic Regression**
- **Archivo:** `03_baseline_LogisticRegression_OneHot_SimpleImputer.ipynb`
- **T√©cnicas:** OneHot Encoding, SimpleImputer
- **Accuracy:** ~35-38%
- **Objetivo:** Establecer l√≠nea base del rendimiento

#### üìà **Modelo 4: Ensemble B√°sico**
- **Archivo:** `04_ensemble_GradientBoosting_XGBoost_LightGBM_OneHot.ipynb`
- **T√©cnicas:** Promedio de probabilidades de 3 modelos
- **Algoritmos:** GradientBoosting, XGBoost, LightGBM
- **Accuracy:** ~40-42%
- **Mejora:** Primera aproximaci√≥n a algoritmos de boosting

#### üéØ **Modelo 5: Stacking Avanzado**
- **Archivo:** `05_stacking_XGB_LGBM_GB_meta_LogReg_OrdinalEncoder.ipynb`
- **T√©cnicas:** Stacking de dos niveles con OOF predictions
- **Base learners:** XGBoost, LightGBM, GradientBoosting
- **Meta-modelo:** Logistic Regression
- **Encoding:** OrdinalEncoder (m√°s eficiente en memoria)
- **Accuracy:** ~42-44%
- **Diferencia clave:** Meta-aprendizaje para combinar modelos de forma inteligente

#### üöÄ **Modelo 6: LightGBM Optimizado**
- **Archivo:** `06_LightGBM_optimizado_OOF_FeatureEngineering_OrdinalEncoder.ipynb`
- **T√©cnicas:** 
  - Validaci√≥n cruzada StratifiedKFold (4-7 folds)
  - Out-of-Fold predictions
  - Early stopping
  - Feature engineering avanzado
- **Hiperpar√°metros optimizados:**
```python
  learning_rate: 0.02
  n_estimators: 2500
  num_leaves: 128
  feature_fraction: 0.8
  bagging_fraction: 0.8
```
- **Accuracy:** ~44%
- **Diferencia clave:** Pas√≥ de "muchos modelos b√°sicos" a "un solo modelo experto bien optimizado"

#### üèÜ **MODELO 99 (FINAL - Entrega 3)**
- **Archivo:** `99_modelo_solucion_ensemble_LGBM_XGB_CatBoost.ipynb`
- **Estrategia:** Ensemble ponderado (Blending)
- **Algoritmos:**
  - **LightGBM** (peso: 0.55) - n_estimators: 600, learning_rate: 0.03
  - **XGBoost** (peso: 0.20) - n_estimators: 550, learning_rate: 0.035
  - **CatBoost** (peso: 0.25) - iterations: 300, learning_rate: 0.06
- **Preprocesamiento especial:**
  - Manejo de NaN diferenciado por algoritmo
  - Categorical features nativas para CatBoost
  - Sentinel values (-9999) para num√©ricos
- **Score final en Kaggle:** **0.44355**
- **Diferencia clave:** Combina tres algoritmos complementarios con pesos optimizados basados en su performance individual

---

### 3. Resultados finales

| M√©trica | Valor |
|---------|-------|
| **Accuracy (Validation)** | 0.4435 |
| **Score Kaggle** | 0.44355 |
| **Mejora vs Baseline** | +6.5% |

**Estrategia ganadora:**
- Diversidad de algoritmos (LightGBM, XGBoost, CatBoost)
- Pesos optimizados basados en performance
- Blending de probabilidades
- Manejo robusto de datos categ√≥ricos

---

## Notas importantes

- El preprocesamiento se mantiene **simple e interpretable**
- Se prioriz√≥ la **reproducibilidad** del c√≥digo
- Todas las transformaciones son **consistentes** entre train y test
- El c√≥digo est√° **documentado** para facilitar la comprensi√≥n
- La evoluci√≥n de modelos muestra un proceso de **aprendizaje iterativo**

---

## Licencia

Este proyecto es de car√°cter acad√©mico y pertenece a la Universidad de Antioquia.

---

**Universidad de Antioquia**  
*Ingenier√≠a de Sistemas - Modalidad Virtual*  
*2025-2*